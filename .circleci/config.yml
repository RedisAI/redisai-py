
# Python CircleCI 2.0 configuration file
#
# Check https://circleci.com/docs/2.0/language-python/ for more details
#
version: 2
jobs:
  build:
    docker:
      - image: circleci/python:3.7.1
      - image: redisai/redisai:edge

    working_directory: ~/repo

    steps:
      - checkout

      - restore_cache: # Download and cache dependencies
          keys:
          - v1-dependencies-{{ checksum "requirements.txt" }}
          # fallback to using the latest cache if no exact match is found
          - v1-dependencies-

      - run:
          name: install dependencies
          command: |
            virtualenv venv
            . venv/bin/activate
            pip install -r requirements.txt
            
      - save_cache:
          paths:
            - ./venv
          key: v1-dependencies-{{ checksum "requirements.txt" }}

      - run:
          name: run tests
          command: |
            echo SPARK_HOME=~/spark-2.4.3-bin-hadoop2.7 >> .bashrc
            echo export PATH=$SPARK_HOME/bin:$PATH >> .bashrc
            source ~/.bashrc
            sudo apt-get update
            sudo apt install openjdk-8-jdk
            wget http://mirrors.estointernet.in/apache/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgz
            sudo tar xvf spark-2.4.3-bin-hadoop2.7.tgz
            . venv/bin/activate
            pip install -r test-requirements.txt
            pip install nose codecov
            nosetests --with-coverage -vsx test
            codecov
            
      - store_artifacts:
          path: test-reports
          destination: test-reports

  build_nightly:
    docker:
      - image: circleci/python:3.7.1
      - image: redisai/redisai:edge

    working_directory: ~/repo

    steps:
      - checkout

      - restore_cache: # Download and cache dependencies
          keys:
          - v1-dependencies-{{ checksum "requirements.txt" }}
          # fallback to using the latest cache if no exact match is found
          - v1-dependencies-

      - run:
          name: install dependencies
          command: |
            virtualenv venv
            . venv/bin/activate
            pip install -r requirements.txt
            
      - save_cache:
          paths:
            - ./venv
          key: v1-dependencies-{{ checksum "requirements.txt" }}

      - run:
          name: run tests
          command: |
            echo SPARK_HOME=~/spark-2.4.3-bin-hadoop2.7 >> .bashrc
            echo export PATH=$SPARK_HOME/bin:$PATH >> .bashrc
            source ~/.bashrc
            sudo apt-get update
            sudo apt install openjdk-8-jdk
            wget http://mirrors.estointernet.in/apache/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgz
            sudo tar xvf spark-2.4.3-bin-hadoop2.7.tgz
            . venv/bin/activate
            pip install -r test-requirements.txt
            pip install nose
            nosetests -vsx test  

      # no need for store_artifacts on nightly builds 

workflows:
  version: 2
  commit:
    jobs:
      - build
  nightly:
    triggers:
      - schedule:
          cron: "0 0 * * *"
          filters:
            branches:
              only:
                - master
    jobs:
      - build_nightly
